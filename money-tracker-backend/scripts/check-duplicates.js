// Script de v√©rification des doublons dans la base de donn√©es PostgreSQL
// Utilisation: node check-duplicates.js
// Pr√©requis: npm install pg dotenv
const pool = require('../config/database');

// Configuration avec les VRAIS noms de colonnes
const TABLES_TO_CHECK = {
  // RISQUE √âLEV√â
  accounts: {
    columns: ['name', 'type', 'user_id'],
    description: 'Comptes avec m√™me nom',
    risk: 'HIGH'
  },
  projects: {
    columns: ['name', 'type', 'status', 'start_date', 'user_id'],
    description: 'Projets dupliqu√©s',
    risk: 'HIGH'
  },
  project_expense_lines: {
    columns: ['project_id', 'description', 'category', 'transaction_date', 'projected_amount'],
    description: 'Lignes de d√©penses dupliqu√©es',
    risk: 'HIGH'
  },
  project_revenue_lines: {
    columns: ['project_id', 'description', 'category', 'transaction_date', 'projected_amount'],
    description: 'Lignes de revenus dupliqu√©es',
    risk: 'HIGH'
  },
  receivables: {
    columns: ['account_id', 'person', 'description', 'amount', 'status'],
    description: 'Cr√©ances dupliqu√©es',
    risk: 'HIGH'
  },
  
  // RISQUE MOYEN
  transactions: {
    columns: ['account_id', 'transaction_date', 'amount', 'type', 'description'],
    description: 'Transactions dupliqu√©es malgr√© contrainte UNIQUE',
    risk: 'MEDIUM'
  },
  tasks: {
    columns: ['title', 'assignee', 'due_date', 'project_id'],
    description: 'T√¢ches dupliqu√©es',
    risk: 'MEDIUM'
  },
  operator_tasks: {
    columns: ['title', 'assignedto', 'duedate', 'projectid'],
    description: 'T√¢ches op√©rateur dupliqu√©es',
    risk: 'MEDIUM'
  },
  sops: {
    columns: ['title', 'owner', 'category', 'project_id'],
    description: 'SOPs dupliqu√©es',
    risk: 'MEDIUM'
  },
  operator_sops: {
    columns: ['title', 'owner', 'category'],
    description: 'SOPs op√©rateur dupliqu√©es',
    risk: 'MEDIUM'
  },
  objectives: {
    columns: ['title', 'category', 'deadline'],
    description: 'Objectifs dupliqu√©s',
    risk: 'MEDIUM'
  },
  master_content: {
    columns: ['title', 'type', 'status'],
    description: 'Contenus master dupliqu√©s',
    risk: 'MEDIUM'
  },
  content_master: {
    columns: ['title', 'type'],
    description: 'Contenus master dupliqu√©s',
    risk: 'MEDIUM'
  },
  derivatives: {
    columns: ['master_id', 'platform', 'format'],
    description: 'D√©riv√©s dupliqu√©s',
    risk: 'MEDIUM'
  },
  content_derivatives: {
    columns: ['master_id', 'platform', 'type'],
    description: 'D√©riv√©s de contenu dupliqu√©s',
    risk: 'MEDIUM'
  },
  
  // RISQUE FAIBLE
  categories: {
    columns: ['name'],
    description: 'Cat√©gories (contrainte UNIQUE)',
    risk: 'LOW'
  },
  employees: {
    columns: ['email'],
    description: 'Employ√©s (contrainte UNIQUE)',
    risk: 'LOW'
  }
};

function buildDuplicateQuery(table, columns) {
  const columnsList = columns.join(', ');
  const groupByColumns = columns.map(col => `${col}`).join(', ');
  
  return `
    SELECT 
      ${columnsList},
      COUNT(*) as duplicate_count,
      ARRAY_AGG(id ORDER BY id) as duplicate_ids
    FROM ${table}
    WHERE ${columns.map(col => `${col} IS NOT NULL`).join(' AND ')}
    GROUP BY ${groupByColumns}
    HAVING COUNT(*) > 1
    ORDER BY COUNT(*) DESC, ${columns[0]};
  `;
}

async function checkDuplicates() {
  const client = await pool.connect();
  const results = {
    summary: { totalTables: 0, tablesWithDuplicates: 0, totalDuplicates: 0 },
    details: []
  };

  try {
    console.log('\nüîç V√âRIFICATION DES DOUBLONS DANS LA BASE DE DONN√âES');
    console.log('='.repeat(70));

    for (const [tableName, config] of Object.entries(TABLES_TO_CHECK)) {
      results.summary.totalTables++;
      
      console.log(`\nüìä Table: ${tableName} [${config.risk}]`);
      console.log(`   Description: ${config.description}`);
      console.log(`   Colonnes: ${config.columns.join(', ')}`);

      try {
        const query = buildDuplicateQuery(tableName, config.columns);
        const result = await client.query(query);

        if (result.rows.length > 0) {
          results.summary.tablesWithDuplicates++;
          const duplicateCount = result.rows.reduce((sum, row) => sum + (row.duplicate_count - 1), 0);
          results.summary.totalDuplicates += duplicateCount;

          console.log(`   ‚ö†Ô∏è  DOUBLONS: ${result.rows.length} groupe(s), ${duplicateCount} doublon(s)`);
          
          result.rows.slice(0, 5).forEach((row, idx) => {
            console.log(`      ${idx + 1}. Count: ${row.duplicate_count}, IDs: [${row.duplicate_ids.join(', ')}]`);
          });

          if (result.rows.length > 5) {
            console.log(`      ... et ${result.rows.length - 5} autre(s) groupe(s)`);
          }

          results.details.push({
            table: tableName,
            risk: config.risk,
            duplicateGroups: result.rows.length,
            totalDuplicates: duplicateCount,
            samples: result.rows.slice(0, 3)
          });
        } else {
          console.log('   ‚úÖ Aucun doublon');
        }
      } catch (error) {
        console.log(`   ‚ùå Erreur: ${error.message}`);
      }
    }

    console.log('\n' + '='.repeat(70));
    console.log('üìã R√âSUM√â');
    console.log('='.repeat(70));
    console.log(`Tables v√©rifi√©es: ${results.summary.totalTables}`);
    console.log(`Tables avec doublons: ${results.summary.tablesWithDuplicates}`);
    console.log(`Total doublons: ${results.summary.totalDuplicates}`);

    if (results.summary.tablesWithDuplicates > 0) {
      console.log('\n‚ö†Ô∏è  ACTIONS RECOMMAND√âES:');
      console.log('   1. Examiner chaque groupe manuellement');
      console.log('   2. Identifier les enregistrements √† conserver');
      console.log('   3. Supprimer les doublons: DELETE FROM table WHERE id = <id>;');
      console.log('   4. Ajouter contraintes UNIQUE si n√©cessaire');
    } else {
      console.log('\n‚úÖ Aucun doublon d√©tect√©!');
    }

    const fs = require('fs');
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, -5);
    const filename = `duplicate-check-${timestamp}.json`;
    fs.writeFileSync(filename, JSON.stringify(results, null, 2));
    console.log(`\nüíæ R√©sultats: ${filename}`);

  } catch (error) {
    console.error('\n‚ùå Erreur fatale:', error);
  } finally {
    client.release();
    await pool.end();
  }
}

console.log('\nüöÄ D√©marrage...');
console.log('‚úÖ Connect√© √† PostgreSQL\n');
checkDuplicates()
  .then(() => {
    console.log('\n‚úÖ V√©rification termin√©e!\n');
    process.exit(0);
  })
  .catch(err => {
    console.error('\n‚ùå Erreur:', err);
    process.exit(1);
  });
